{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -d ./ ../alexa-prize-topical-chat-dataset/conversations.zip\n",
    "!unzip -d ./ ../alexa-prize-topical-chat-dataset/reading_sets.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### assign entity to each utterance\n",
    "The entity need to be post-processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of dialog:  8628\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import copy\n",
    "import pandas as pd\n",
    "topic = {1:'Fashion',2:'Politics', 3:'Books', 4:'Sports',5:'General Entertainment',6:'Music',7:'Science & Technology', 8:'Movie'}\n",
    "assignment = pd.read_csv('entity_topic_assign.csv')\n",
    "entity_assign = dict()\n",
    "for index, row in assignment.iterrows():\n",
    "    tmp = row['topic']\n",
    "    tmp = topic[int(tmp)]\n",
    "    entity_assign[row['entity']] = tmp\n",
    "\n",
    "\n",
    "src1 = 'conversations/train.json'\n",
    "src2 = 'reading_sets/post-build/train.json'\n",
    "with open(src1,'r') as f, open(src2,'r') as g:\n",
    "    t1 = json.load(f)\n",
    "    t2 = json.load(g)\n",
    "dataset = []\n",
    "print('Total number of dialog: ',len(t1.keys()))\n",
    "num1=0\n",
    "for i in t1.keys():\n",
    "    num1 = num1+1\n",
    "    t1[i].pop(\"article_url\")\n",
    "    t1[i].pop('config')\n",
    "    t1[i].pop('conversation_rating')\n",
    "    for j in range(len(t1[i]['content'])):\n",
    "        agent = t1[i]['content'][j]['agent']\n",
    "        source = t1[i]['content'][j]['knowledge_source']\n",
    "        t1[i]['content'][j]['entity'] = []\n",
    "        t1[i]['content'][j]['topic'] = []\n",
    "        source = set(source)\n",
    "        set_fs = set(['FS1','FS2','FS3'])\n",
    "        set_as = set(['AS1','AS2','AS3'])\n",
    "        if set_fs&source!=set():\n",
    "            for k in list(set_fs&source):\n",
    "                t1[i]['content'][j]['entity'].append(t2[i][agent][k]['entity'])\n",
    "        elif set_fs&source==set() and set_as&source!=set():\n",
    "            for k in list(set_as&source):\n",
    "                tmp = t2[i]['article'][k]\n",
    "                t1[i]['content'][j]['entity'].append(\"No entity: id: \"+i+'; '+k+': '+tmp)\n",
    "    for v in range(len(t1[i]['content'])):\n",
    "        num = v\n",
    "        if t1[i]['content'][v]['entity']==[]:\n",
    "            set1 = set(t1[i]['content'][num-1]['knowledge_source'])&set(['FS1','FS2','FS3'])\n",
    "            if num<len(t1[i]['content'])-1:\n",
    "                set2 = set(t1[i]['content'][num+1]['knowledge_source'])&set(['FS1','FS2','FS3'])\n",
    "            else:\n",
    "                set2 = set(t1[i]['content'][num-1]['knowledge_source'])&set(['FS1','FS2','FS3'])\n",
    "            if len(set1&set2)!=0:\n",
    "                source = [x for x in set1&set2]\n",
    "                for u in range(len(source)):\n",
    "                    t1[i]['content'][v]['entity'].append(t2[i][t1[i]['content'][v][\"agent\"]][source[u]]['entity'])\n",
    "                t1[i]['content'][v]['entity'].append('INFER')\n",
    "            else:\n",
    "                tmp = []\n",
    "                source = [x for x in set1|set2]\n",
    "                if len(source)>0:\n",
    "                    for u in range(len(source)):\n",
    "                        tmp.append(t2[i][t1[i]['content'][v][\"agent\"]][source[u]]['entity'])\n",
    "                    t1[i]['content'][v]['entity'] = [\"No entity: topic change\",tmp]\n",
    "                else:\n",
    "                    t1[i]['content'][v]['entity'] = [\"No entity: cannot infer from former and latter\"]\n",
    "    dataset.append([])\n",
    "    dataset[-1] = copy.deepcopy(t1[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### label topics with Rule 1, 2, 3, 4, 6. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# for utterance without topics, the assign the name entity to that appear the most. If a dialog don't have topic, assign all topics of entities\n",
    "for i in range(len(dataset)):\n",
    "    tmp = {}\n",
    "    for k in range(len(dataset[i]['content'])):\n",
    "        if 'No entity' in dataset[i]['content'][k]['entity'][0]:\n",
    "            continue\n",
    "        for j in range(len(dataset[i]['content'][k]['entity'])):\n",
    "            if dataset[i]['content'][k]['entity'][j]=='INFER':\n",
    "                pass\n",
    "            else:\n",
    "                #print(dataset[i]['content'][k]['entity'])\n",
    "                try:\n",
    "                    dataset[i]['content'][k]['topic'].append(entity_assign[dataset[i]['content'][k]['entity'][j]])\n",
    "                except:\n",
    "                    print(dataset[i]['content'][k]['entity'])\n",
    "                if entity_assign[dataset[i]['content'][k]['entity'][j]] not in tmp:\n",
    "                    tmp[entity_assign[dataset[i]['content'][k]['entity'][j]]] = 1\n",
    "                else:\n",
    "                    tmp[entity_assign[dataset[i]['content'][k]['entity'][j]]] = tmp[entity_assign[dataset[i]['content'][k]['entity'][j]]]+1\n",
    "    if tmp!={}:\n",
    "        max_key = max(tmp, key=tmp.get)\n",
    "        keys = []\n",
    "        for v in tmp:\n",
    "            if tmp[v]==tmp[max_key]:\n",
    "                keys.append(v)\n",
    "    else:\n",
    "        keys = []\n",
    "        for k in range(len(dataset[i]['content'])):\n",
    "            if 'No entity: id: ' in dataset[i]['content'][k]['entity'][0]:\n",
    "                idx = re.findall(r'No entity: id: (.+); AS',dataset[i]['content'][k]['entity'][0])\n",
    "        for tt in ['FS1','FS2','FS3']:\n",
    "            keys.append(entity_assign[t2[idx[0]]['agent_1'][tt]['entity']])\n",
    "        '''\n",
    "        print(i)\n",
    "        print(json.dumps(dataset[i]['content'],indent=4))\n",
    "        max_key = max(tmp, key=tmp.get)\n",
    "        keys = []\n",
    "        for v in tmp:\n",
    "            if v==max_key:\n",
    "                keys.append(v)\n",
    "        '''\n",
    "    for k in range(len(dataset[i]['content'])):\n",
    "        if dataset[i]['content'][k]['topic']==[]:\n",
    "            if dataset[i]['content'][k]['entity'][0]=='No entity: topic change': \n",
    "                for j in dataset[i]['content'][k]['entity'][1]:\n",
    "                    dataset[i]['content'][k]['topic'].append(entity_assign[j])\n",
    "            else:\n",
    "                for v in keys:\n",
    "                    dataset[i]['content'][k]['topic'].append(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save csv file and reload them to list. Check bugs for file encoding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = []\n",
    "NER = []\n",
    "topic = []\n",
    "for i in dataset:\n",
    "    for j in i['content']:\n",
    "        sentence.append(j['message'])\n",
    "        NER.append(j['entity'])\n",
    "        topic.append(j['topic'])\n",
    "data = {'sentence':sentence,'entity':NER,'topic':topic}\n",
    "df = pd.DataFrame(data)  \n",
    "df.to_csv('train.csv',encoding='utf-8')\n",
    "\n",
    "train = pd.read_csv('train.csv', index_col=0)\n",
    "\n",
    "train = train[['sentence','entity','topic']]\n",
    "\n",
    "import ast\n",
    "all_entity = train['entity'].values.tolist()\n",
    "for i in range(len(all_entity)):\n",
    "    try:\n",
    "        tmp = copy.copy(all_entity[i])\n",
    "        all_entity[i] = ast.literal_eval(all_entity[i])\n",
    "    except:\n",
    "        print(tmp)\n",
    "\n",
    "all_topic = train['topic'].values.tolist()\n",
    "p = 0\n",
    "for i in range(len(all_topic)):\n",
    "    try:\n",
    "        p = p+1\n",
    "        tmp = copy.copy(all_topic[i])\n",
    "        all_topic[i] = ast.literal_eval(all_topic[i])\n",
    "    except:\n",
    "        print(p)\n",
    "        print(tmp)\n",
    "\n",
    "message = train['sentence'].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label topics with Rule 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['topic_with_general'] = None\n",
    "train['topic_as'] = None\n",
    "topic_as = []\n",
    "number3 = 0 # count the number of adjusting AS to previous appeared entity\n",
    "number4 = 0 # count the number of all AS\n",
    "for i in range(len(all_entity)):\n",
    "    tmp = all_entity[i]\n",
    "    t1 = True\n",
    "    t2 = True\n",
    "    if 'No entity: id: ' in tmp[0]:\n",
    "        number4+=1\n",
    "        k1 = i-1\n",
    "        k2 = i+1\n",
    "        if 'No entity: cannot infer from former and latter' in all_entity[k1][0] or 'No entity: id: ' in all_entity[k1][0]:\n",
    "            t1 = False\n",
    "        if 'No entity: cannot infer from former and latter' in all_entity[k2][0] or 'No entity: id: ' in all_entity[k2][0]:\n",
    "            t2 = False\n",
    "        if t1 or t2:\n",
    "            number3+=1\n",
    "            topic_as.append(list(set(all_topic[k1]+all_topic[k2])))\n",
    "            #print(list(set(all_topic[k1]+all_topic[k2])))\n",
    "            #print(tmp)\n",
    "            #print(all_entity[k1])\n",
    "            #print(all_entity[k2])\n",
    "        else:\n",
    "            topic_as.append(list(set(all_topic[i])))\n",
    "            #print(list(set(all_topic[k1]+all_topic[k2])))\n",
    "            #print(tmp)\n",
    "            #print(all_entity[k1])\n",
    "            #print(all_entity[k2])\n",
    "    else:\n",
    "        topic_as.append(list(set(all_topic[i])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There are 6997 utterances are label with Rule 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6997, 14279)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number3,number4 # count the number of adjusting AS to previous appeared entity, count the number of all AS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label general topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get the first utterance and last 2 utterances index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import copy\n",
    "import pandas as pd\n",
    "src1 = 'conversations/train.json'\n",
    "src2 = 'reading_sets/post-build/train.json'\n",
    "with open(src1,'r') as f, open(src2,'r') as g:\n",
    "    t1 = json.load(f)\n",
    "    t2 = json.load(g)\n",
    "num1 = -1\n",
    "num2 = []\n",
    "for i in t1.keys():\n",
    "    #num1 = num1+1\n",
    "    #print(t1[i])\n",
    "    t1[i].pop(\"article_url\")\n",
    "    t1[i].pop('config')\n",
    "    for j in range(len(t1[i]['content'])):\n",
    "        num1 = num1+1\n",
    "        if j<1 or j>(len(t1[i]['content'])-3):\n",
    "            num2.append(num1)\n",
    "num2 = set(num2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "group utterances in conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import copy\n",
    "import pandas as pd\n",
    "src1 = 'conversations/train.json'\n",
    "src2 = 'reading_sets/post-build/train.json'\n",
    "with open(src1,'r') as f, open(src2,'r') as g:\n",
    "    t1 = json.load(f)\n",
    "    t2 = json.load(g)\n",
    "s1 = -1\n",
    "s2 = []\n",
    "for i in t1.keys():\n",
    "    #num1 = num1+1\n",
    "    #print(t1[i])\n",
    "    t1[i].pop(\"article_url\")\n",
    "    t1[i].pop('config')\n",
    "    temp = []\n",
    "    for j in range(len(t1[i]['content'])):\n",
    "        s1 = s1+1\n",
    "        #if j<1 or j>(len(t1[i]['content'])-3):\n",
    "        temp.append(s1)\n",
    "    s2.append([])\n",
    "    s2[-1] = copy.copy(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label general topic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_with_general = []\n",
    "change = []\n",
    "ext = {'Fashion':['fashion'],\n",
    "       'Politics':['politics','us','united state','vote','republican','govern',\"gov't\",'obama','leader','senate','summit','democratic'], \n",
    "       'Books':['book','shakespeare','isaac asimov','robot','read','stan lee','libraries','library','hero','character','novel'], \n",
    "       'Sports':['sport','nba'],\n",
    "       'General Entertainment':['game','entertain'],\n",
    "       'Music':['rap','sing','song','music'],\n",
    "       'Science & Technology':['ocean','water','earth','programming language'], \n",
    "       'Movie':['movie','imdb','superhero','actor','Oscars','film']}\n",
    "for i in range(len(all_entity)):\n",
    "    if i in num2:\n",
    "        if 'No entity: cannot infer from former and latter' in all_entity[i][0] or 'No entity: id: ' in all_entity[i][0]:\n",
    "            topic_with_general.append(topic_as[i])\n",
    "        elif 'No entity: topic change'==all_entity[i][0]:\n",
    "            check = False\n",
    "            tmp_sent = message[i].lower()\n",
    "            # {1:'Fashion',2:'Politics', 3:'Books', 4:'Sports',5:'General Entertainment',6:'Music',7:'Science & Technology', 8:'Movie'}\n",
    "            ext_temp = []\n",
    "            for x in topic_as[i]:\n",
    "                ext_temp = ext_temp+ext[x]\n",
    "            for j in all_entity[i][1]+ext_temp:\n",
    "                j = j.lower()\n",
    "                j=j.split('(')[0].strip()\n",
    "                temp = j.split()\n",
    "                temp = list(filter(lambda y:y!='',temp))\n",
    "                for u in temp:\n",
    "                    if u in tmp_sent:\n",
    "                        check = True\n",
    "            if check==False:\n",
    "                topic_with_general.append(['General'])\n",
    "                change.append(i)\n",
    "            else:\n",
    "                topic_with_general.append(topic_as[i])\n",
    "        else:\n",
    "            check = False\n",
    "            tmp_sent = str(message[i]).lower()\n",
    "            ext_temp = []\n",
    "            for x in topic_as[i]:\n",
    "                ext_temp = ext_temp+ext[x]\n",
    "            for j in all_entity[i]+ext_temp:\n",
    "                j = j.lower()\n",
    "                j=j.split('(')[0].strip()\n",
    "                temp = j.split()\n",
    "                temp = list(filter(lambda y:y!='',temp))\n",
    "                for u in temp:\n",
    "                    if u in tmp_sent:\n",
    "                        check = True\n",
    "            if check==False:\n",
    "                topic_with_general.append(['General'])\n",
    "                change.append(i)\n",
    "            else:\n",
    "                topic_with_general.append(topic_as[i])\n",
    "    else:\n",
    "        topic_with_general.append(topic_as[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save middle result to train_v4.csv. The topic label is ready to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['topic_as'] = topic_as\n",
    "train['topic_with_general'] = topic_with_general\n",
    "\n",
    "train.to_csv('train_v4.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### post process entitiy label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### post process rule 2 and rule 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_en=[]\n",
    "entity_en=[]\n",
    "topic_en=[]\n",
    "topic_as_en=[]\n",
    "topic_general_en=[]\n",
    "check = []\n",
    "rule2 = []\n",
    "for i in s2:\n",
    "    for j in i:\n",
    "        if 'No entity: topic change' in all_entity[j][0]:\n",
    "            check.append(j)\n",
    "            message_en.append(copy.copy(message[j]))\n",
    "            entity_en.append(list(set(all_entity[j][1])))\n",
    "            topic_en.append(copy.copy(all_topic[j]))\n",
    "            topic_as_en.append(copy.copy(topic_as[j]))\n",
    "            topic_general_en.append(copy.copy(topic_with_general[j]))\n",
    "        elif 'INFER' in all_entity[j]:\n",
    "            check.append(j)\n",
    "            rule2.append(j)\n",
    "            message_en.append(copy.copy(message[j]))\n",
    "            entity_en.append(copy.copy(all_entity[j][:-1]))\n",
    "            topic_en.append(copy.copy(all_topic[j]))\n",
    "            topic_as_en.append(copy.copy(topic_as[j]))\n",
    "            topic_general_en.append(copy.copy(topic_with_general[j]))\n",
    "        else:\n",
    "            check.append(j)\n",
    "            message_en.append(copy.copy(message[j]))\n",
    "            entity_en.append(copy.copy(all_entity[j]))\n",
    "            topic_en.append(copy.copy(all_topic[j]))\n",
    "            topic_as_en.append(copy.copy(topic_as[j]))\n",
    "            topic_general_en.append(copy.copy(topic_with_general[j]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### post process rule 4,5,6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caculate entity frequency in each conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_freq = [[] for i in range(len(s2))]\n",
    "entity_except = []\n",
    "for i in range(len(s2)):\n",
    "    tmp = {}\n",
    "    for j in s2[i]:\n",
    "        if 'No entity: ' not in all_entity[j][0] and 'INFER' != all_entity[j][-1]:\n",
    "            for k in all_entity[j]:\n",
    "                if k not in tmp:\n",
    "                    tmp[k] = 1\n",
    "                if k in tmp:\n",
    "                    tmp[k]+=1\n",
    "    if tmp=={}:\n",
    "        entity_except.append(i)\n",
    "        entity_freq[i] = []\n",
    "    else:\n",
    "        max_key = max(tmp, key=tmp.get)\n",
    "        keys = []\n",
    "        for v in tmp:\n",
    "            if tmp[v]==tmp[max_key]:\n",
    "                keys.append(v)\n",
    "        entity_freq[i] = keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### implement post process for rule 4, 5, 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_rule4_0 = []\n",
    "check_rule4_1 = [[],[]]\n",
    "check_rule3 = []\n",
    "entity2 = copy.deepcopy(entity_en)\n",
    "number3 = 0 # count the number of adjusting AS to previous appeared entity\n",
    "number4 = 0 # count the number of all AS\n",
    "for i in range(len(entity_en)):\n",
    "    tmp = all_entity[i]\n",
    "    t1 = True\n",
    "    t2 = True\n",
    "    if 'No entity: id: ' in tmp[0]:\n",
    "        number4+=1\n",
    "        k1 = i-1\n",
    "        k2 = i+1\n",
    "        if 'No entity: cannot infer from former and latter' in all_entity[k1][0] or 'No entity: id: ' in all_entity[k1][0]:\n",
    "            t1 = False\n",
    "        if 'No entity: cannot infer from former and latter' in all_entity[k2][0] or 'No entity: id: ' in all_entity[k2][0]:\n",
    "            t2 = False\n",
    "        if t1 or t2:\n",
    "            # Rule5\n",
    "            number3+=1\n",
    "            check_rule4_1[0].append(i)\n",
    "            \n",
    "            temp2 = list(set(entity2[k1]+entity2[k2]))\n",
    "            temp2 = list(filter(lambda x:'No entity:'not in x,temp2))\n",
    "            entity2[i] = copy.copy(temp2)\n",
    "        else:\n",
    "            # Rule6\n",
    "            check_rule4_1[1].append(i)\n",
    "            for idx, diag in enumerate(s2):\n",
    "                if i in diag:\n",
    "                    entity2[i] = copy.copy(entity_freq[idx])\n",
    "                    break\n",
    "    elif 'No entity: cannot infer from former and latter' in tmp[0]:\n",
    "        # Rule4\n",
    "        check_rule4_0.append(i)\n",
    "        for idx, diag in enumerate(s2):\n",
    "            if i in diag:\n",
    "                entity2[i] = copy.copy(entity_freq[idx])\n",
    "                break\n",
    "    elif 'No entity: topic change' in tmp[0]:\n",
    "        # Rule3\n",
    "        check_rule3.append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save as a whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = {'sentence':message_en,'entity_ori':all_entity,'entity':entity2,'topic':topic_as_en,'topic_with_general':topic_general_en}\n",
    "data = {'sentence':message_en,'entity':entity2,'topic_with_general':topic_general_en}\n",
    "pd.DataFrame(data).to_csv('train_final.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save dataset according to their rule's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41864"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rule2)+len(check_rule3)+len(check_rule4_0)+len(check_rule4_1[0])+len(check_rule4_1[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "rule1 = []\n",
    "infer_set = set(rule2+check_rule3+check_rule4_0+check_rule4_1[0]+check_rule4_1[1])\n",
    "for i in range(len(all_entity)):\n",
    "    if i not in infer_set:\n",
    "        rule1.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(146514, 188378)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rule1),len(rule1)+len(rule2)+len(check_rule3)+len(check_rule4_0)+len(check_rule4_1[0])+len(check_rule4_1[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = [rule1, rule2, check_rule3, check_rule4_0, check_rule4_1[0], check_rule4_1[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rule 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx1 = idx[0]\n",
    "message1 = []\n",
    "entity1 = []\n",
    "topic1 = []\n",
    "for i in idx1:\n",
    "    message1.append(message_en[i])\n",
    "    entity1.append(entity2[i])\n",
    "    topic1.append(topic_general_en[i])\n",
    "data = {'id':idx1,'sentence':message1,'entity':entity1,'topic':topic1}\n",
    "pd.DataFrame(data).to_csv('train_final_rule1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rule 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx1 = idx[1]\n",
    "message1 = []\n",
    "entity1 = []\n",
    "topic1 = []\n",
    "for i in idx1:\n",
    "    message1.append(message_en[i])\n",
    "    entity1.append(entity2[i])\n",
    "    topic1.append(topic_general_en[i])\n",
    "data = {'id':idx1,'sentence':message1,'entity':entity1,'topic':topic1}\n",
    "pd.DataFrame(data).to_csv('train_final_rule2.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rule 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx1 = idx[2]\n",
    "message1 = []\n",
    "entity1 = []\n",
    "topic1 = []\n",
    "for i in idx1:\n",
    "    message1.append(message_en[i])\n",
    "    entity1.append(entity2[i])\n",
    "    topic1.append(topic_general_en[i])\n",
    "data = {'id':idx1,'sentence':message1,'entity':entity1,'topic':topic1}\n",
    "pd.DataFrame(data).to_csv('train_final_rule3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rule 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx1 = idx[3]\n",
    "message1 = []\n",
    "entity1 = []\n",
    "topic1 = []\n",
    "for i in idx1:\n",
    "    message1.append(message_en[i])\n",
    "    entity1.append(entity2[i])\n",
    "    topic1.append(topic_general_en[i])\n",
    "data = {'id':idx1,'sentence':message1,'entity':entity1,'topic':topic1}\n",
    "pd.DataFrame(data).to_csv('train_final_rule4.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rule 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx1 = idx[4]\n",
    "message1 = []\n",
    "entity1 = []\n",
    "topic1 = []\n",
    "for i in idx1:\n",
    "    message1.append(message_en[i])\n",
    "    entity1.append(entity2[i])\n",
    "    topic1.append(topic_general_en[i])\n",
    "data = {'id':idx1,'sentence':message1,'entity':entity1,'topic':topic1}\n",
    "pd.DataFrame(data).to_csv('train_final_rule5.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rule 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx1 = idx[5]\n",
    "message1 = []\n",
    "entity1 = []\n",
    "topic1 = []\n",
    "for i in idx1:\n",
    "    message1.append(message_en[i])\n",
    "    entity1.append(entity2[i])\n",
    "    topic1.append(topic_general_en[i])\n",
    "data = {'id':idx1,'sentence':message1,'entity':entity1,'topic':topic1}\n",
    "pd.DataFrame(data).to_csv('train_final_rule6.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use word2vec embedding similarity to filter out labels that are not trustable\n",
    "#### This code propose another entity and topic labeling method using RAKE and word2vec embedding. Only labels that are agreed with both methods will be saved. \n",
    "We use word2vec embeddings from https://github.com/plasticityai/magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘filter’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pymagnitude==0.1.143\n",
    "!pip install rake-nltk==1.0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp train_final_rule*.csv filter/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wget: missing URL\r\n",
      "Usage: wget [OPTION]... [URL]...\r\n",
      "\r\n",
      "Try `wget --help' for more options.\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir filter/word2vec;cd filter/word2vec; wget http://magnitude.plasticity.ai/word2vec/medium/GoogleNews-vectors-negative300.magnitude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rule2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd filter;python keywords_ext.py train_final_rule2.csv r2_keywords.txt;python gold_label_classifier.py train_final_rule2.csv r2_keywords.txt r2_topic_agreement.csv r2_entity_agreement.csv ./word2vec/GoogleNews-vectors-negative300.magnitude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rule 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd filter;python keywords_ext.py train_final_rule3.csv r3_keywords.txt;python gold_label_classifier.py train_final_rule3.csv r3_keywords.txt r3_topic_agreement.csv r3_entity_agreement.csv ./word2vec/GoogleNews-vectors-negative300.magnitude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rule 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd filter;python keywords_ext.py train_final_rule4.csv r4_keywords.txt;python gold_label_classifier.py train_final_rule4.csv r4_keywords.txt r4_topic_agreement.csv r4_entity_agreement.csv ./word2vec/GoogleNews-vectors-negative300.magnitude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rule 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd filter;python keywords_ext.py train_final_rule5.csv r5_keywords.txt;python gold_label_classifier.py train_final_rule5.csv r5_keywords.txt r5_topic_agreement.csv r5_entity_agreement.csv ./word2vec/GoogleNews-vectors-negative300.magnitude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rule 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd filter;python keywords_ext.py train_final_rule6.csv r6_keywords.txt;python gold_label_classifier.py train_final_rule6.csv r6_keywords.txt r6_topic_agreement.csv r6_entity_agreement.csv ./word2vec/GoogleNews-vectors-negative300.magnitude"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}