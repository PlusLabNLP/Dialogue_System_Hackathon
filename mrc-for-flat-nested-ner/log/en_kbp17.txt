Please notice that merge the args_dict and json_config ... ...
{
  "hidden_size": 768,
  "clip_grad": 1,
  "bert_config": {
    "attention_probs_dropout_prob": 0.1,
    "hidden_act": "gelu",
    "hidden_dropout_prob": 0.1,
    "hidden_size": 768,
    "initializer_range": 0.02,
    "intermediate_size": 3072,
    "max_position_embeddings": 512,
    "num_attention_heads": 12,
    "num_hidden_layers": 12,
    "type_vocab_size": 2,
    "vocab_size": 30522
  },
  "config_path": "/home/shannon/mrc-for-flat-nested-ner/config/en_bert_base_uncased.json",
  "data_dir": "/home/shannon/dataset/en_kbp17",
  "bert_model": "/home/shannon/BERT_BASE_DIR/uncased_L-12_H-768_A-12",
  "task_name": null,
  "max_seq_length": 170,
  "train_batch_size": 36,
  "dev_batch_size": 36,
  "test_batch_size": 36,
  "checkpoint": 800,
  "learning_rate": 3.5e-05,
  "num_train_epochs": 10,
  "warmup_proportion": -1.0,
  "local_rank": -1,
  "gradient_accumulation_steps": 1,
  "seed": 2333,
  "export_model": false,
  "output_dir": "/home/shannon/export_dir/mrc-ner/kbp17/mrc-2020.06.03-170-3.5e-5-36-18_1",
  "data_sign": "kbp17",
  "weight_start": 0.9,
  "weight_end": 0.9,
  "weight_span": 1.0,
  "entity_sign": "nested",
  "n_gpu": 3,
  "dropout": 0.2,
  "entity_threshold": 0.5,
  "data_cache": false
}
-*--*--*--*--*--*--*--*--*--*-
current data_sign: kbp17
=*==*==*==*==*==*==*==*==*==*=
loading train data ... ...
40580
40580 train data loaded
=*==*==*==*==*==*==*==*==*==*=
loading dev data ... ...
6482
6482 dev data loaded
=*==*==*==*==*==*==*==*==*==*=
loading test data ... ...
6774
6774 test data loaded
######################################################################
EPOCH:  0
/home/shannon/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
current training loss is : 
0.012051330879330635
/home/shannon/venv/lib/python3.6/site-packages/torch/nn/functional.py:1386: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.
  warnings.warn("nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.")
............................................................
DEV: loss, acc, precision, recall, f1
0.0075 0.9986 0.589 0.5776 0.5832
............................................................
TEST: loss, acc, precision, recall, f1
0.0069 0.9988 0.5953 0.578 0.5865
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
current training loss is : 
0.001957313623279333
............................................................
DEV: loss, acc, precision, recall, f1
0.0069 0.9988 0.6125 0.5855 0.5987
............................................................
TEST: loss, acc, precision, recall, f1
0.0063 0.9989 0.6081 0.601 0.6046
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
######################################################################
EPOCH:  1
current learning rate 1.9e-05
current learning rate 1.9e-05
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
current training loss is : 
0.007903531193733215
............................................................
DEV: loss, acc, precision, recall, f1
0.0054 0.999 0.6398 0.5926 0.6153
............................................................
TEST: loss, acc, precision, recall, f1
0.0048 0.9992 0.6523 0.6296 0.6408
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
current training loss is : 
0.0010740236612036824
............................................................
DEV: loss, acc, precision, recall, f1
0.006 0.9991 0.6062 0.6497 0.6272
............................................................
TEST: loss, acc, precision, recall, f1
0.0053 0.9992 0.6116 0.6746 0.6415
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
######################################################################
EPOCH:  2
current learning rate 1.805e-05
current learning rate 1.805e-05
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
current training loss is : 
0.005303212441504002
............................................................
DEV: loss, acc, precision, recall, f1
0.006 0.999 0.6492 0.6483 0.6487
............................................................
TEST: loss, acc, precision, recall, f1
0.005 0.9992 0.6502 0.6841 0.6667
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
current training loss is : 
0.0003729007439687848
............................................................
DEV: loss, acc, precision, recall, f1
0.0076 0.999 0.6618 0.6593 0.6606
............................................................
TEST: loss, acc, precision, recall, f1
0.0063 0.9992 0.6773 0.6891 0.6831
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
######################################################################
EPOCH:  3
current learning rate 1.71475e-05
current learning rate 1.71475e-05
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
current training loss is : 
0.003631284460425377
............................................................
DEV: loss, acc, precision, recall, f1
0.0066 0.999 0.6908 0.7211 0.7056
............................................................
TEST: loss, acc, precision, recall, f1
0.0054 0.9992 0.7065 0.7424 0.724
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
current training loss is : 
0.00010893261060118675
............................................................
DEV: loss, acc, precision, recall, f1
0.0078 0.9991 0.7028 0.6723 0.6872
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
######################################################################
EPOCH:  4
current learning rate 1.6290125e-05
current learning rate 1.6290125e-05
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
current training loss is : 
0.0015621983911842108
............................................................
DEV: loss, acc, precision, recall, f1
0.0074 0.9991 0.6767 0.6728 0.6747
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
current training loss is : 
9.008582856040448e-05
............................................................
DEV: loss, acc, precision, recall, f1
0.0084 0.9991 0.71 0.7434 0.7263
............................................................
TEST: loss, acc, precision, recall, f1
0.0071 0.9993 0.7121 0.7759 0.7426
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
######################################################################
EPOCH:  5
current learning rate 1.547561875e-05
current learning rate 1.547561875e-05
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
current training loss is : 
0.00473451241850853
............................................................
DEV: loss, acc, precision, recall, f1
0.0084 0.9991 0.7091 0.7092 0.7092
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
current training loss is : 
5.671835606335662e-05
............................................................
DEV: loss, acc, precision, recall, f1
0.0093 0.9991 0.701 0.7511 0.7252
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
######################################################################
EPOCH:  6
current learning rate 1.47018378125e-05
current learning rate 1.47018378125e-05
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
current training loss is : 
0.0017818573396652937
............................................................
DEV: loss, acc, precision, recall, f1
0.0098 0.9991 0.7256 0.7183 0.7219
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
current training loss is : 
8.288782555609941e-05
............................................................
DEV: loss, acc, precision, recall, f1
0.0092 0.9991 0.7169 0.7511 0.7336
............................................................
TEST: loss, acc, precision, recall, f1
0.0079 0.9992 0.6977 0.7735 0.7337
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
######################################################################
EPOCH:  7
current learning rate 1.3966745921874999e-05
current learning rate 1.3966745921874999e-05
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
current training loss is : 
0.0030049982015043497
............................................................
DEV: loss, acc, precision, recall, f1
0.0098 0.9991 0.6961 0.7418 0.7183
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
current training loss is : 
4.971361340722069e-05
............................................................
DEV: loss, acc, precision, recall, f1
0.0096 0.9991 0.7086 0.764 0.7353
............................................................
TEST: loss, acc, precision, recall, f1
0.0083 0.9993 0.7088 0.7867 0.7457
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
######################################################################
EPOCH:  8
current learning rate 1.3268408625781248e-05
current learning rate 1.3268408625781248e-05
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
current training loss is : 
0.00047024499508552253
............................................................
DEV: loss, acc, precision, recall, f1
0.0092 0.9991 0.7193 0.7514 0.735
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
current training loss is : 
8.948944741860032e-05
............................................................
DEV: loss, acc, precision, recall, f1
0.0098 0.9935 0.7828 0.8094 0.7959
............................................................
TEST: loss, acc, precision, recall, f1
0.0063 0.8959 0.7454 0.8895 0.8111
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
######################################################################
EPOCH:  9
current learning rate 1.2604988194492186e-05
current learning rate 1.2604988194492186e-05
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
current training loss is : 
6.834427040303126e-05
............................................................
DEV: loss, acc, precision, recall, f1
0.0119 0.9935 0.7952 0.7828 0.7889
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
current training loss is : 
5.029829844716005e-05
............................................................
DEV: loss, acc, precision, recall, f1
0.0123 0.9934 0.7854 0.7801 0.7827
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
=&==&==&==&==&==&==&==&==&==&==&==&==&==&==&=
Best DEV : overall best loss, acc, precision, recall, f1 
0.0098 0.9935 0.7828 0.8094 0.7959
scores on TEST when Best DEV:loss, acc, precision, recall, f1 
0.0063 0.8959 0.7454 0.8895 0.8111
=&==&==&==&==&==&==&==&==&==&==&==&==&==&==&=